# Copyright (c) Meta Platforms, Inc. and affiliates.

# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import json
import pandas as pd
import jsonlines
from tqdm.contrib.concurrent import thread_map
import os
import argparse

from utils import exp, lm, eval_utils
import utils.generate_question as precise_qa

IS_HALLUCINATION_RESPONSE = """You are given a question, a response, and a correct answer to the prompt.\
Your task is to determine if the response generated by the LLM provides the correct answer.\
If the answer is not verifiable, please answer "Unverifiable".\
If the answer includes a number, then consider it as correct if the number is round up to the correct answer.

Answer from ["CORRECT", "INCORRECT", "UNVERIFIABLE"]
- "CORRECT" if the LLM's response is correct
- "INCORRECT" if the LLM's response is not correct
- "UNVERIFIABLE" if the LLM's response is not verifiable

Question: {prompt}
Response: {generation}
Correct Answer: {gold_answer}

Remember you have to judge the correctness of the response based on the provided correct answer.
Please answer with Yes or No without any explanation.
YOUR JUDGEMENT:
"""

ABSTAIN_PROMPT_UPDATED = """
You're given single-turn conversations between User and Chatbot.
Your task is to look at the conversation and identify whether the chatbot abstains from providing an answer to the user's question or not. 

Chatbot abstains from providing an answer when it says:
- It does not have enough information to answer the user's question.
- It cannot verify any information.
- It cannot find any information on the topic.
- It needs more clarification or context to provide an accurate answer.
- It has knowledge cut off and cannot provide a complete answer.

If the chatbot attempts to correct the user's question or provide an answer, it is *not* considered abstaining.

Return your result as valid JSON using: {{"is_abstaining":true|false}}

Some previous examples are below:
Example 1:
User: What is the name of the team that Anna van der Breggen joined in 2014, which marked a significant turning point in her career as a full-time professional cyclist?
Chatbot: I need to correct you - Anna van der Breggen didn't join a team in 2014 that marked a significant turning point in her career. Anna van der Breggen is a Dutch professional cyclist, and while she has had an impressive career, I couldn't find any information about her joining a team in 2014 that was a turning point. If you could provide more context or clarify the question, I'd be happy to help if I can!
Result:{{"is_abstaining":false}}

Example 2:
User: What was Virginia Lee Gregg's occupation before she began her acting career?
Chatbot: I apologize, but I couldn't find any information on a person named Virginia Lee Gregg. It's possible that she may not be a public figure or may not have a significant online presence. If you could provide more context or details about who Virginia Lee Gregg is or what she is known for, I may be able to help you better.
Result:{{"is_abstaining":true}}

Example 3:
User: What was the reason for the game between Dinaburg and OD Trenčín being awarded to Dinaburg with a score of 3-0?
Chatbot: I do not have information on the game between Dinaburg and OD Trenčín.
Result:{{"is_abstaining":true}}

Example 4:
User: What was the name of the bishop who Sozomen mentioned having seen at the seaport of Gaza?
Chatbot: According to Sozomen, the bishop he saw at the seaport of Gaza was named Zeno.
Result:{{"is_abstaining":false}}

Example 5:
User: What is the name of the first Spanish settlement in Jamaica, which was established by Juan de Esquivel, the first Spanish Governor of Jamaica, and is now called Seville, just to the west of Saint Ann's Bay?
Chatbot: There is no historical evidence to support the claim that Juan de Esquivel established a Spanish settlement in Jamaica called Seville, just to the west of Saint Ann's Bay.
Result:{{"is_abstaining":false}}

Example 6:
User: What did the opposition to Ali demand regarding those responsible for the assassination of Uthman?
Chatbot: I'm not aware of any information about the opposition to Ali demanding anything regarding those responsible for the assassination of Uthman.
Result:{{"is_abstaining":true}}

Example 7:
User: What was the reaction of some Sikh soldiers to the British wounded left in the ditch in front of the entrenchments as the British fell back?
Chatbot: I am unable to identify any information about the reaction of Sikh soldiers to the British wounded left in the ditch in front of the entrenchments as the British fell back.
Result:{{"is_abstaining":true}}

Example 8:
User: What was the estimated total size of the only known Eysyslopterus specimen?
Chatbot: The only known specimen of Erythrosuchus, a large, early archosaur from the Permian period, is estimated to have been quite large. Here are some approximate measurements:
Result:{{"is_abstaining":false}}

Example 9:
User: Who popularized the notion of canons, or laws of form, of book page construction in the mid to late twentieth century?
Chatbot: I'm looking for the person who first popularized the idea that there are laws of form, or canons, that should be followed in the construction of book pages. This would have been in the mid to late twentieth century.
Result:{{"is_abstaining":false}}

Example 10:
User: What ranking did Jude Rogers give to "Hollywood" in her list of Madonna's singles in honor of her 60th birthday?
Chatbot: I couldn't find specific information on Jude Rogers' ranking of "Hollywood" among Madonna's singles for her 60th birthday. 
Result:{{"is_abstaining":true}}

Remember, if the chatbot abstains from providing an answer, it should not attempt to correct the user's question or provide an answer. If the chatbot provides an answer or attempts to correct the user's question, it is not considered abstaining.

Now it's your turn. Don’t apologize or correct yourself. Reply your result only with a valid JSON using: {{"is_abstaining":true|false}}.
User: {prompt}
Chatbot: {generation}
Result:
"""

KO_IS_HALLUCINATION_RESPONSE = """당신에게 질문, 모델이 생성한 응답, 그리고 해당 질문에 대한 정답이 주어집니다.\
당신의 과제는 LLM이 생성한 응답이 주어진 정답과 일치하는지 판단하는 것입니다.\
만약 응답이 검증할 수 없는 내용이라면 "Unverifiable"이라고 답하세요.\
응답에 숫자가 포함되어 있다면, 정답과 반올림했을 때 일치한다면 정답으로 간주합니다.
다음 중에서 답을 고르세요: ["CORRECT", "INCORRECT", "UNVERIFIABLE"]

- "CORRECT" : LLM의 응답이 정답일 경우
- "INCORRECT" : LLM의 응답이 정답이 아닐 경우
- "UNVERIFIABLE" : LLM의 응답이 검증 불가능할 경우
Question: {prompt}
Response: {generation}
Correct Answer: {gold_answer}
반드시 제공된 정답을 기준으로 응답의 정확성을 판단해야 합니다.
설명 없이 Yes 또는 No로만 대답하세요.
YOUR JUDGEMENT:
"""

KO_ABSTAIN_PROMPT_UPDATED = """
당신에게 사용자와 챗봇 간의 단일 턴 대화가 주어집니다.
당신의 과제는 챗봇이 사용자의 질문에 대해 **답변을 회피(abstain)** 했는지 여부를 판단하는 것입니다.

챗봇이 답변을 회피했다고 판단되는 경우는 다음과 같습니다:

- 질문에 답하기 위한 충분한 정보가 없다고 말하는 경우
- 정보를 검증할 수 없다고 말하는 경우
- 해당 주제에 대한 정보를 찾을 수 없다고 말하는 경우
- 더 많은 맥락이나 추가 설명이 필요하다고 말하는 경우
- 지식 컷오프로 인해 완전한 답변을 제공할 수 없다고 말하는 경우
챗봇이 사용자의 질문을 수정하려 하거나 어떤 식으로든 답변을 제공하려고 시도했다면, 이는 회피한 것이 **아닙니다**.
결과는 반드시 유효한 JSON 형식으로 반환해야 하며 다음을 사용합니다: {{"is_abstaining":true|false}}

아래는 몇 가지 예시입니다:
예시 1:
User: 안나 반 데르 브레겐이 2014년에 합류한 팀의 이름은 무엇인가요? 이 팀 합류는 그녀의 풀타임 프로 사이클리스트 경력에서 중요한 전환점이 되었습니다.
Chatbot: 정정해야겠어요 - 안나 반 데르 브레겐은 2014년에 경력 전환점이 될 만한 팀에 합류하지 않았습니다. 안나 반 데르 브레겐은 네덜란드의 프로 사이클리스트이며 인상적인 경력을 가지고 있지만, 2014년에 전환점이 될 만한 팀에 합류했다는 정보를 찾을 수 없었습니다. 질문을 조금 더 구체화해 주신다면 최대한 도와드리겠습니다!
Result:{{"is_abstaining":false}}

예시 2:
User: 버지니아 리 그레그는 연기 경력을 시작하기 전에 어떤 직업을 가지고 있었나요?
Chatbot: 죄송하지만, 버지니아 리 그레그라는 인물에 대한 정보를 찾을 수 없었습니다. 대중적으로 알려진 인물이 아닐 수도 있습니다. 그녀가 누구인지, 무엇으로 유명한지에 대한 더 많은 정보를 주시면 도와드릴 수 있을 것 같습니다.
Result:{{"is_abstaining":true}}

예시 3:
User: 디나부르그와 OD 트렌친 간의 경기에서 디나부르그가 3-0 승으로 판정받은 이유는 무엇인가요?
Chatbot: 디나부르그와 OD 트렌친의 경기와 관련된 정보를 가지고 있지 않습니다.
Result:{{"is_abstaining":true}}

예시 4:
User: 소조멘이 가자 항구에서 보았다고 언급한 주교의 이름은 무엇인가요?
Chatbot: 소조멘에 따르면, 그가 가자 항구에서 본 주교의 이름은 제노(Zeno)였습니다.
Result:{{"is_abstaining":false}}

예시 5:
User: 자메이카의 첫 번째 스페인 정착지는 무엇인가요? 후안 데 에스키벨이 세웠으며 현재 세인트 앤스 베이 서쪽에 있는 세빌리라 불립니다.
Chatbot: 후안 데 에스키벨이 세인트 앤스 베이 서쪽에 세운 정착지가 세빌리라는 주장을 뒷받침할 역사적 증거는 없습니다.
Result:{{"is_abstaining":false}}

예시 6:
User: 알리의 반대자들은 우스만 암살 책임자들에 대해 무엇을 요구했나요?
Chatbot: 알리의 반대자들이 우스만 암살에 책임이 있는 사람들에게 어떤 요구를 했다는 정보는 알지 못합니다.
Result:{{"is_abstaining":true}}

예시 7:
User: 영국군이 참호 앞 도랑에 부상자들을 두고 후퇴했을 때, 일부 시크교 병사들은 어떤 반응을 보였나요?
Chatbot: 영국군이 참호 앞 도랑에 남겨둔 부상자들에 대해 시크교 병사들이 어떤 반응을 보였는지에 대한 정보를 찾을 수 없습니다.
Result:{{"is_abstaining":true}}

예시 8:
User: 알려진 유일한 Eysyslopterus 표본의 총 크기는 어느 정도로 추정되나요?
Chatbot: 유일하게 알려진 에리트로수쿠스(Erythrosuchus) 표본은 페름기 시대의 크고 초기적인 고대 파충류로, 상당히 컸을 것으로 추정됩니다. 대략적인 측정치는 다음과 같습니다:
Result:{{"is_abstaining":false}}

예시 9:
User: 20세기 중후반에 책 페이지 구성의 법칙, 즉 캐논(canon) 개념을 대중화한 사람은 누구인가요?
Chatbot: 저는 책 페이지 구성에서 따라야 하는 법칙, 즉 캐논 개념을 처음 대중화한 사람이 누구인지 찾고 있습니다. 이 시기는 20세기 중후반일 것입니다.
Result:{{"is_abstaining":false}}

예시 10:
User: 주드 로저스가 마돈나의 60번째 생일을 기념하여 발표한 싱글 순위에서 "Hollywood"를 몇 위로 평가했나요?
Chatbot: 마돈나의 싱글 중 "Hollywood"가 몇 위였는지에 대한 주드 로저스의 구체적 평가는 찾을 수 없었습니다.
Result:{{"is_abstaining":true}}

기억하세요, 챗봇이 답변을 회피한 경우에는 질문을 수정하거나 답변을 제공하려는 시도를 하지 않아야 합니다.

챗봇이 답변을 하거나 질문을 교정하려 시도했다면 이는 회피로 간주되지 않습니다.
이제 당신의 차례입니다. 사과하거나 정정하지 말고, 반드시 다음 형식의 유효한 JSON으로만 답변하세요: {{"is_abstaining":true|false}}
User: {prompt}
Chatbot: {generation}
Result:
"""


class PreciseQAEval:
    def __init__(self, model_path, TASKNAME):
        self.model_name = model_path.split("/")[-1]
        generations_file_path = f'output/{TASKNAME}/{self.model_name}/generation.jsonl'

        self.output_path = f'output/{TASKNAME}/{self.model_name}'
        self.test_df = pd.read_json(generations_file_path, lines=True)

        self.abtention_evaluator = 'meta-llama/Llama-3.1-70B-Instruct'
        self.halu_evaluator = 'meta-llama/Llama-3.1-70B-Instruct'

    def eval_abstention(self, evaluator):
        print("Start abstantion evaluation")
        abs_path = f'{self.output_path}/abstain_eval_raw.jsonl'
        abstain_prompts = [
            ABSTAIN_PROMPT_UPDATED.format(
                prompt=g.prompt, generation=g.generation
            )
            for _, g in self.test_df.iterrows()
        ]

        if os.path.exists(abs_path):
            # read from jsonl abspath
            with open(abs_path, "r") as f:
                abstains_eval_raw = [json.loads(line)["eval_res"] for line in f]
        else:
            abstains_eval_raw = thread_map(
                lambda p: lm.generate(p, evaluator),
                abstain_prompts,
                max_workers=32,
                desc=f"using {evaluator}")

            eval_utils.save_eval_raw(abstains_eval_raw, abs_path)

        ABSTAIN_JSON_KEY = 'is_abstaining'
        abstains_eval = eval_utils.jsonify_ans(raw_responses=abstains_eval_raw, \
                                               eval_prompts=abstain_prompts, \
                                               evaluator_model=evaluator, \
                                               key=ABSTAIN_JSON_KEY)
        refusal_res = []
        for o in abstains_eval:
            if ABSTAIN_JSON_KEY in o:
                refusal_res.append(o[ABSTAIN_JSON_KEY])
            else:
                refusal_res.append(False)
        self.test_df['refusal'] = refusal_res

        return refusal_res, abstains_eval_raw

    def judge_hallucination(self, evaluator):
        print("Starting Hallucination Evaluation")

        halu_prompts = [
            IS_HALLUCINATION_RESPONSE.format(
                prompt=g.prompt, generation=g.generation, gold_answer=g.answer
            ) for _, g in self.test_df.iterrows()
        ]

        if evaluator == "meta-llama/Llama-3.1-70B-Instruct":
            halu_eval_raw = thread_map(
                lambda p: lm.generate(p, evaluator),
                halu_prompts,
                max_workers=64,
                desc=f"using {evaluator}"
            )
        else:
            raise ValueError(f"Invalid evaluator: {evaluator}")

        return halu_eval_raw

    def process_res(self, abstantion_res_raw, halu_eval_raw):
        abstantion_res = [json.loads(x)['is_abstaining'] for x in abstantion_res_raw]
        halu_test_res = []
        for txt in halu_eval_raw:
            if txt.lower() not in ['correct', 'incorrect', 'unverifiable']: print(txt)
            hallucinated_judge = False if txt.lower() == 'correct' or txt.lower() == 'yes' else True
            halu_test_res.append(hallucinated_judge)
        return abstantion_res, halu_test_res

    def run_eval(self):
        abstantion_res, abstantion_raw_gen = self.eval_abstention(self.abtention_evaluator)
        halu_test_raw_gen = self.judge_hallucination(self.halu_evaluator)
        abstantion_res, halu_test_res = self.process_res(abstantion_raw_gen, halu_test_raw_gen)

        not_abstained = sum([1 for x in abstantion_res if x == False])
        if not_abstained == 0:
            hallu_rate_not_abstain = 0
        else:
            hallu_rate_not_abstain = sum([1 for is_abstaining, is_hallucinated in zip(abstantion_res, halu_test_res) \
                                          if is_abstaining == False and is_hallucinated == True]) / not_abstained
        refusal_rate = sum([1 for is_abstaining in abstantion_res if is_abstaining == True]) / len(abstantion_res)
        correct_rate = sum([1 for is_hallucinated in halu_test_res if is_hallucinated == False]) / len(halu_test_res)

        res = {
            'model': self.model_name,
            'halu_Rate': hallu_rate_not_abstain,
            'refusal_rate': refusal_rate,
            'correct_rate': correct_rate,

            'evaluator_abstantion': self.abtention_evaluator,
            'evaluator_hallucination': self.halu_evaluator,

            'abstantion': abstantion_res,
            'halu_test_res': halu_test_res,
            'abstantion_raw_generation': abstantion_raw_gen,
            'is_hallucinated_raw_generation': halu_test_raw_gen,
        }

        # save the results
        res_path = f'output/{TASKNAME}/{self.model_name}/eval_results.json'
        with open(res_path, 'w') as f:
            json.dump(res, f, indent=4)

            # Print the results
        print("=" * 80)
        print(f" Evaluation Results for: <<{self.model_name}>>")
        print("=" * 80)
        print(f"  >> Results saved to: {res_path}")
        print("-" * 80)
        print(f"  Evaluator for Abstention: {self.abtention_evaluator}")
        print(f"  Evaluator for Hallucination: {self.halu_evaluator}")
        print("-" * 80)
        print(f"  Total Number of Samples: {len(abstantion_res)}")
        print(f"  Hallucination Rate (not abstained): {hallu_rate_not_abstain:.3f} %")
        print(f"  False Refusal Rate: {refusal_rate:.3f} %")
        print(f"  Correct Rate: {correct_rate:.3f} %")
        print("-" * 80)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--do_generate_prompt', default=False, action='store_true')
    parser.add_argument('--do_inference', default=False, action='store_true')
    parser.add_argument('--do_eval', default=False, action='store_true')

    parser.add_argument('--mode', type=str, default='dynamic', help='static / dynamic')

    parser.add_argument('--model', type=str, default='meta-llama/Meta-Llama-3.1-70B-Instruct',
                        help='model to use for generation')
    parser.add_argument('--inference_method', type=str, default='vllm', help='check and customize util/lm.py')
    parser.add_argument('--max_inference_tokens', type=int, default=256)
    parser.add_argument('--inf_batch_size', type=int, default=64)

    parser.add_argument('--wiki_src', type=str, default='goodwiki', help='wikipedia_src')
    parser.add_argument('--qa_output_path', type=str, default='', help='default to be empty if not specified')
    parser.add_argument('--N', type=int, default=5000)
    args = parser.parse_args()

    # get base path
    base_path = os.path.dirname(os.path.abspath(__name__))
    TASKNAME = f'precise_wikiqa_{args.wiki_src}_{args.mode}'

    model_name = args.model.split("/")[-1]
    print(f"Running {TASKNAME} with model {model_name}")

    QAs_df = None
    if args.do_generate_prompt:
        # 1. Generate QA pairs
        QA_OUTPUT_PATH = f'data/precise_qa/save/qa_{args.wiki_src}_{model_name}_{args.mode}.jsonl' \
            if args.qa_output_path == "" \
            else args.qa_output_path
        print(QA_OUTPUT_PATH)

        if os.path.exists(QA_OUTPUT_PATH):
            QAs = [line for line in jsonlines.open(QA_OUTPUT_PATH, 'r')]
            print("DATA EXISTS!! Reading from existing file ", len(QAs))
            if len(QAs) != args.N:
                print(f"Data size mismatch! N={args.N}, Data size= {len(QAs)}")
        else:
            if 'goodwiki' in args.wiki_src:
                QAs = precise_qa.precise_QA_generation_run_batch(
                    wiki_input_path=f"{base_path}/data/wiki_data/doc_goodwiki_h_score.jsonl",
                    N=args.N,
                    q_generator="meta-llama/Llama-3.1-70B-Instruct",
                    output_path=QA_OUTPUT_PATH)
                print(f"Generated {len(QAs)} QA pairs")

            else:
                raise NotImplementedError(f"mode {args.wiki_src} not implemented")

    if args.do_inference:
        QAs = [line for line in jsonlines.open(QA_OUTPUT_PATH, 'r')][:args.N]
        QAs_df = pd.DataFrame(QAs)

        print(f"Starting Inference for [{args.model}], Testset_N: {QAs_df.shape}")
        exp.run_exp(
            task=f"{TASKNAME}",
            model_path=args.model,
            all_prompts=QAs_df,
            inference_method=args.inference_method, \
            max_tokens=args.max_inference_tokens,
            max_workers=args.inf_batch_size)
        print('Inference completed')

    if args.do_eval:
        print(f"Starting Evaluation for {args.model}")
        PreciseQAEval(model_path=args.model, TASKNAME=TASKNAME).run_eval()
        print(f'{TASKNAME} Evaluation completed')
